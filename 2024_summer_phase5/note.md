# DatawhaleAI夏令营笔记

## Task1

### 机器学习基础

#### 内涵

机器具备学习的能力，即让机器具备**找一个函数**的能力



#### 分类

##### 回归

通过函数输出一个数值

如预测未来某个时间的PM2.5



##### 分类

让机器做选择

如判断邮件是否为垃圾邮件



##### 结构化学习

产生一个有机构的物体

如让机器画一张画





#### 找函数的步骤

##### 写出一个带有未知参数的函数

如 `y = wx + b`

x: **Feature**

w: **weight**

b: **bias**



##### 定义损失

###### 损失函数

`L = L(b,w)`



###### 平均绝对误差（MAE)

```latex
e = |yˆ − y|
```



###### 均方误差（MSE）

```latex
e = (yˆ − y)2
```





##### 最优化

###### 梯度下降

![f1](images/f1.png)



假设这里有一个损失函数

L = L（w）

随机假设一个点w0，可以计算函数在该点处的微分

```latex
w1 ← w0 − η∂L/∂w|w=w0
```

从中可以发现斜率绝对值越大，移动的步伐越大，斜率为正向左移，为负向右移，不断迭代可以找到一个极值点，称为**局部最小值**，但不一定是**全局最小值**，这与w0有关



η：**学习率**

影响步伐大小，由自己设定，称为**超参数**



## Task2

### 线性模型

#### 概念

把输入的特征 x 乘上一个权重，再加上一个偏置得到预测的结果



#### HardSigmoid

![f2](images\f2.png)

**特性**：Hard Sigmoid 函数的特性是当输入的值，当 x 轴的值小于某一个阈值（某个定值）的时候，大于另外一个定值阈值的时候，中间有一个斜坡。所以它是先水平的，再斜坡，再水平的。





#### 分段线性曲线

但是如果只是单纯的线性模型，函数终归只是一条斜线，无法变得曲折，因此我们引入**分段线性曲线**

通过分段线性曲线，我们可以将折线图像化作若干HardSigmod函数的和

同理对于曲线图像，我们可以在曲线上取若干个点，化曲为直

![f3](images\f3.png)





#### Sigmoid

HardSigmoid函数并不是很好写，因此我们可以用Sigmoid函数来逼近

sigmod：

$\ y = c\frac{1}{1+e^{-(b+wx)}} $

![f4](images\f4.png)



![f5](images\f5.png)





因此y可以写作

$ y = b+ \sum_{i}^{n}c_{i}\frac{1}{1+e^{-(b_{i}+w_{i}x)}} $



#### 扩展到多个特征

即由前一天扩展到前j天

![f6](images\f6.png)



以前三天为例,i表示目标函数由多少个sigmoid函数组成，j表示参考前多少天

$$
r_{1} = b_{1} + w_{11}x_{1} + w_{12}x_{2} + w_{13}x_{3}\\
r_{2} = b_{2} + w_{21}x_{1} + w_{22}x_{2} + w_{23}x_{3}\\
r_{3} = b_{3} + w_{31}x_{1} + w_{32}x_{2} + w_{33}x_{3}
$$


化为矩阵
$$
\begin{bmatrix}
r_{1}\\
r_{2}\\
r_{3}
\end{bmatrix}=
\begin{bmatrix}
b_{1}\\
b_{2}\\
b_{3}
\end{bmatrix}+
\begin{bmatrix}
w_{11}\ w_{12}\ w_{13} \\
w_{21}\ w_{22}\ w_{23}\\
w_{31}\ w_{32}\ w_{33}
\end{bmatrix}
\begin{bmatrix}
x_{1}\\
x_{2}\\
x_{3}
\end{bmatrix}
$$
即
$$
\mathbf{r = b + Wx}
$$
令
$$
\mathbf{a = \sigma(r)}
$$
有
$$
y = b + \mathbf{c^{T}a}
$$




#### 定义损失

之前是简单的`L = L(b,w)`，而现在有许多参数，因此用  $\theta $ 来代替这些未知参数

因此有 $ \ L = L(\theta)$ 

接着和上面一样，随机选择一组 $\ \theta$ ，

令

$\ \mathbf{g = \bigtriangledown L(\theta)}$

然后梯度下降,更新参数

$\ \mathbf{\theta_{1} \leftarrow \theta_{0} - \eta g}$

最后停下来，得到让损失最小的一组，称为$\ \theta ^{*}$





#### 批量与回合

实际使用梯度下降时，并不是利用所有数据生成一个loss，而是把N笔数据随机分成一个一个的批量（batch），每个批量有B笔数据，

每个批量可以得到$\ L_{1},L_{2} ···$，当B足够大时，$\ L_{1}$可能会与$\ L$接近

对所有批次进行一次梯度下降叫做回合，对某一批进行一次梯度下降叫做更新



#### 模型变形

不一定要把hardsigmoid变成softsigmoid，也可以将其换为两个修正线性单元（ReLU)的加总

$\ ReLU = c*max(0,b+wx)$

![f8](images\f8.png)



##### 激活函数

像sigmoid，ReLU ···



#### 神经网络

##### 改进模型

![f9](images\f9.png)

(Q:为什么是把a带到x的位置)





sigmoid或relu称为神经元，很多神经元称为神经网络，每一排称为隐藏层，越叠越深称为深度学习



##### 过拟合

假如我每叠一层，测试集的loss就变小，而当某一次，再叠一层时，loss变大了，此时称为过拟合





